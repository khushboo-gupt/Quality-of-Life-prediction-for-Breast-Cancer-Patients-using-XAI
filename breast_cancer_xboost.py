# -*- coding: utf-8 -*-
"""Breast cancer_XBoost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c6FetpTSzs5uakcnLzzTH-E-CjISSmWB
"""

import pandas  as pd

dataset_path = '/content/sample_data/Breast Cancer METABRIC.csv'
df = pd.read_csv(dataset_path)

df = df.dropna()
df

df = df.drop("Patient ID", axis=1)
df.tail()

import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import xgboost as xgb

# Assuming your dataset is named cancer_data
# Extract feature and target arrays
X = df.drop(['Overall Survival (Months)', "Patient's Vital Status"], axis=1)
y_regression = df['Overall Survival (Months)']
y_classification = df["Patient's Vital Status"]

# Extract categorical columns
cats = X.select_dtypes(exclude=np.number).columns.tolist()

# Convert categorical columns to Pandas category
for col in cats:
   X[col] = X[col].astype('category')

# Split the data
X_train, X_test, y_reg_train, y_reg_test, y_cls_train, y_cls_test = train_test_split(
    X, y_regression, y_classification, random_state=1
)

from sklearn.preprocessing import LabelEncoder

# Create a label encoder instance
label_encoder = LabelEncoder()

# Fit and transform the training labels
y_cls_train_encoded = label_encoder.fit_transform(y_cls_train)

# Transform the test labels
y_cls_test_encoded = label_encoder.transform(y_cls_test)

# Create XGBoost DMatrix for classification
dtrain_cls = xgb.DMatrix(X_train, label=y_cls_train_encoded, enable_categorical=True)
dtest_cls = xgb.DMatrix(X_test, label=y_cls_test_encoded, enable_categorical=True)

# Create XGBoost DMatrix for regression
dtrain_reg = xgb.DMatrix(X_train, label=y_reg_train, enable_categorical=True)
dtest_reg = xgb.DMatrix(X_test, label=y_reg_test, enable_categorical=True)

# # Create XGBoost DMatrix for classification
# dtrain_cls = xgb.DMatrix(X_train, label=y_cls_train, enable_categorical=True)
# dtest_cls = xgb.DMatrix(X_test, label=y_cls_test, enable_categorical=True)

# Define XGBoost parameters for classification
params_cls = {
    'objective': 'multi:softmax',  # Specify the multi-class classification objective
    'num_class': 3,  # Number of classes in your target variable
    'eval_metric': 'mlogloss'  # Use multi-logloss as the evaluation metric
}

# Train the XGBoost model
num_rounds = 100  # You can adjust this based on your dataset and performance
model_cls = xgb.train(params_cls, dtrain_cls, num_rounds)

# Make predictions on the test set
y_pred_cls = model_cls.predict(dtest_cls)

# Convert the predicted labels back to original labels
y_pred_cls_original = label_encoder.inverse_transform(y_pred_cls.astype(int))

y_pred_cls_original

from sklearn.metrics import accuracy_score

# Convert the true test labels back to original labels
y_true_cls_original = label_encoder.inverse_transform(y_cls_test_encoded.astype(int))

# Calculate accuracy
accuracy_cls = accuracy_score(y_true_cls_original, y_pred_cls_original)
print(f"Classification Accuracy: {accuracy_cls:.2f}")

from sklearn.metrics import classification_report, confusion_matrix

# Display classification report
class_report_cls = classification_report(y_true_cls_original, y_pred_cls_original)
print("Classification Report:\n", class_report_cls)

# Display confusion matrix
conf_matrix_cls = confusion_matrix(y_true_cls_original, y_pred_cls_original)
print("Confusion Matrix:\n", conf_matrix_cls)

